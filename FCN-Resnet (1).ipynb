{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a2b054c-0c12-42ed-84af-bc7c966e0fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio.transforms as T\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import librosa\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc7b0fa1-7608-4b93-bcb6-44f2c466f345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d700070d-116d-437f-ba0c-a05e7f7299ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "fs = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a04b5ba-9599-4730-ad24-7c56bfbab458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.conda/envs/torch-2.4.1/lib/python3.11/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (300) may be set too high. Or, the value for `n_freqs` (1025) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_mean = 2.5853811548103334\n",
    "train_std = 48.60284136954955\n",
    "normalize_t = torchvision.transforms.Normalize(mean=train_mean, std=train_std)\n",
    "\n",
    "\n",
    "class ReconstructedFakeSoundDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None, split=\"train\", **transform_args):\n",
    "        self.split = split\n",
    "        self.data_dir = data_dir\n",
    "        with fs.open(f\"{data_dir}/meta_data/{split}_reconstructed.json\") as m:\n",
    "            d = json.load(m)\n",
    "            self.metadata = d[\"audios\"]\n",
    "        self.transform = transform\n",
    "        self.precision = 0.01\n",
    "        \n",
    "        self.transform_args = transform_args\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        m = self.metadata[idx]\n",
    "        filepath = m[\"filepath\"]\n",
    "        label = m[\"label\"]\n",
    "        filepath = f\"s3://bukovec-ml-data/FakeAudio/{filepath[25:]}\"\n",
    "        audio = None\n",
    "        sample_rate = None\n",
    "        # with fs.open(filepath) as d:\n",
    "        #     audio, sample_rate = torchaudio.load(d)\n",
    "        #     if sample_rate != 32000:\n",
    "        #         # print(f\"WARNING - audio {m['audio_id']} resampled from {sample_rate} to 32khz\")\n",
    "        #         audio = torchaudio.functional.resample(audio, sample_rate, 32000)\n",
    "        #         sample_rate = 32000\n",
    "        with fs.open(filepath) as d:\n",
    "            audio, sample_rate = librosa.load(d, sr=32000)\n",
    "        onset, offset = (float(x) for x in m[\"onset_offset\"].split(\"_\"))\n",
    "        segment = torch.zeros(int(10 / self.precision))\n",
    "        segment[int(onset / self.precision): int(offset/self.precision)] = 1.0\n",
    "        if self.transform:\n",
    "            audio = self.transform(torch.tensor(audio).unsqueeze(0), sample_rate, **self.transform_args)\n",
    "        # print(f\"{audio_name} {audio_path} {label}\")\n",
    "        return (audio, torch.tensor(label, dtype=torch.float32), segment, {\n",
    "            \"onset\": onset,\n",
    "            \"offset\": offset,\n",
    "            \"audio_id\": m[\"audio_id\"]\n",
    "        })\n",
    "    \n",
    "mel_spectrogram_32khz = T.MelSpectrogram(\n",
    "        sample_rate=32000,\n",
    "        n_fft=2048,\n",
    "        hop_length=320,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm=\"slaney\",\n",
    "        n_mels=300,\n",
    "        mel_scale=\"htk\",\n",
    "        f_min=0,\n",
    "        f_max = 32000/2\n",
    "    )\n",
    "\n",
    "def transform(audio, sample_rate, n_fft=2048, hop_length=160, n_mels=300, win_length=None):\n",
    "    if sample_rate == 32000:\n",
    "        m = mel_spectrogram_32khz\n",
    "    else:\n",
    "        m = T.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=2048,\n",
    "        win_length=None,\n",
    "        hop_length=320,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm=\"slaney\",\n",
    "        n_mels=300,\n",
    "        mel_scale=\"htk\",\n",
    "        f_min=0,\n",
    "        f_max = sample_rate/2\n",
    "    )\n",
    "    # s = spectrogram(audio)\n",
    "    s = m(audio)\n",
    "    s = normalize_t(s)\n",
    "    # this truncates the last 0.005 seconds, but idrc\n",
    "    s = F.pad(s, (0, 1000 - s.shape[2]))\n",
    "    # if s.shape != (1, 300, 2000):\n",
    "    #     print(\"ruh roh\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66e383a3-b89b-4418-91a9-d0dc1561b5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ReconstructedFakeSoundDataset('s3://bukovec-ml-data/FakeAudio', transform=transform, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "721af88d-a282-423e-8310-30d836fadbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 300, 1000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe19e673-05da-4167-bf0c-dda8f6a2e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetunedMobileNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.channel_up = nn.Conv2d(1, 3, kernel_size=1, stride=1, padding=0)\n",
    "        self.backbone = torchvision.models.segmentation.deeplabv3_mobilenet_v3_large(weights='DEFAULT')\n",
    "        self.backbone.classifier[4] = nn.Conv2d(256, 1, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        self.classifier = nn.Linear(300 * 1000, 1)\n",
    "        self.segmentation = nn.Sequential(nn.Dropout(p=0.3), nn.Linear(300 * 1000, 1000), nn.ReLU(), nn.Dropout(p=0.3), nn.Linear(1000, 1000))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.channel_up(x)\n",
    "        x = self.backbone(x)['out']\n",
    "        x = x.view(B, 1 * H * W)\n",
    "        c = self.classifier(x)\n",
    "        c = torch.sigmoid(c)\n",
    "        s = self.segmentation(x)\n",
    "        s = torch.sigmoid(s)\n",
    "        return c, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7f720db-455b-4e0d-b9c3-be67aa726786",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FinetunedMobileNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5efe0d01-bddd-490e-be46-403289e4a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cf617a7-4b0e-4e73-8d45-42e509e052ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdc7d0b0-d91f-48ae-b73d-ae3e49cfa5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "class_criterion = nn.BCELoss()\n",
    "# segmentation_criterion = torchvision.ops.sigmoid_focal_loss\n",
    "segmentation_criterion = nn.BCELoss()\n",
    "alpha = 0.3 # class loss weight \n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "model.train()\n",
    "writer = SummaryWriter()\n",
    "for epoch in range(10):  \n",
    "    print(f\"Epoch {epoch}\")\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels, segments, _ = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        segments = segments.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        labels_, segments_ = model(inputs)\n",
    "        class_loss = class_criterion(labels_.view(-1), labels)\n",
    "        # segmentation_loss = segmentation_criterion(segments_, segments, alpha=0.4)\n",
    "        segmentation_loss = segmentation_criterion(segments_, segments)\n",
    "        \n",
    "        loss = alpha * class_loss + (1-alpha) * segmentation_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar(\"Batch loss/train\", loss.item(), epoch * len(train_loader) + i)\n",
    "        running_loss += loss.item()\n",
    "            \n",
    "    writer.add_scalar(\"Loss/train\", loss.item(), epoch)\n",
    "    writer.add_scalar(\"Running loss/epoch\", running_loss, epoch)\n",
    "    writer.flush()\n",
    "        \n",
    "print('Finished Training')\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e378a38a-1603-4f89-8381-5d9d21be2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mobilenet-baseline.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e91b89e4-0a6c-4c5a-9158-ea068fd1632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_set = ReconstructedFakeSoundDataset('s3://bukovec-ml-data/FakeAudio', split='easy', transform=transform)\n",
    "hard_set = ReconstructedFakeSoundDataset('s3://bukovec-ml-data/FakeAudio', split='hard', transform=transform)\n",
    "dev_set = torch.utils.data.ConcatDataset([easy_set, hard_set])\n",
    "dev_loader = torch.utils.data.DataLoader(dev_set, batch_size=16)\n",
    "test_set = ReconstructedFakeSoundDataset('s3://bukovec-ml-data/FakeAudio', split='zeroshot', transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e46038-e2f6-41ba-ba14-9ff76c0eee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a5ed172-1fca-4078-a4f1-f7d123185cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Accuracy = 200 / 343 = 0.5830904245376587, Segment Precision = 0.1332918795375101, Segment Recall = 0.9899648019171722. Segment F1 = 0.23494944886974603\n",
      "Score = 0.3393917381763458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.3394, device='cuda:0'),\n",
       " tensor(0.5831, device='cuda:0'),\n",
       " 0.1332918795375101,\n",
       " 0.9899648019171722,\n",
       " 0.23494944886974603)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def segmentation_metrics(pred, target):\n",
    "    tp = (pred * target).sum().item()\n",
    "    fp = ((pred == 1) & (target == 0)).sum().item()\n",
    "    fn = ((pred == 0) & (target == 1)).sum().item()\n",
    "\n",
    "    return tp, fp, fn\n",
    "    \n",
    "def eval_model(model, dataset):\n",
    "    easy_loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "    model.eval()\n",
    "    class_correct = 0\n",
    "    n_samples = 0\n",
    "    seg_tp = 0\n",
    "    seg_fp = 0 \n",
    "    seg_fn = 0\n",
    "    with torch.no_grad():\n",
    "        for j, vdata in enumerate(easy_loader, 0):\n",
    "            vinputs, vclass, vseg, _ = vdata\n",
    "            vinputs = vinputs.to(device)\n",
    "            vclass = vclass.to(device)\n",
    "            vseg = vseg.to(device)\n",
    "            vclass_, vseg_ = model(vinputs)\n",
    "            vseg_ = torch.sigmoid(vseg_)\n",
    "            vclass_ = (vclass_.view(-1) > 0.5).float()\n",
    "            vseg_ = (vseg_ > 0.5).float()\n",
    "            class_correct += (vclass_ == vclass).sum()\n",
    "            n_samples += vclass_.shape[0]\n",
    "            tp, fp, fn = segmentation_metrics(vseg_, vseg)\n",
    "            seg_tp += tp\n",
    "            seg_fp += fp\n",
    "            seg_fn += fn\n",
    "\n",
    "    class_acc = class_correct/n_samples\n",
    "    seg_prec = seg_tp / (seg_tp + seg_fp)\n",
    "    seg_recall = seg_tp / (seg_tp + seg_fn)\n",
    "    seg_f1 = 2 / ((1 / seg_prec) + (1/seg_recall))\n",
    "    score = alpha * class_acc + (1-alpha) * seg_f1\n",
    "    print(f\"Class Accuracy = {class_correct} / {n_samples} = {class_acc.item()}, Segment Precision = {seg_prec}, Segment Recall = {seg_recall}. Segment F1 = {seg_f1}\")\n",
    "    print(f\"Score = {score}\")\n",
    "\n",
    "    return score, class_acc, seg_prec, seg_recall, seg_f1\n",
    "\n",
    "eval_model(model, dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf492544-77d9-4700-99de-811043907cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Accuracy = 125 / 257 = 0.48638132214546204, Segment Precision = 0.12295171064431099, Segment Recall = 0.9493285774234158. Segment F1 = 0.21770720558161916\n",
      "Score = 0.29830944538116455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.2983, device='cuda:0'),\n",
       " tensor(0.4864, device='cuda:0'),\n",
       " 0.12295171064431099,\n",
       " 0.9493285774234158,\n",
       " 0.21770720558161916)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782eb92e-3a87-423e-bc88-a855d2cb9bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-2.4.1",
   "language": "python",
   "name": "torch-2.4.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
